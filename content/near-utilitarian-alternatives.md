---
title: "Near-Utilitarian Alternatives"
date: 2022-08-08T10:10:53-04:00
draft: false
menu: "main"
weight: 7
image: "/img/Utilitarianism-Website-Logo.png"
---


{{< TOC >}}


## Introduction

Consequentialism holds that _promoting good outcomes_ is what matters morally. Utilitarianism is [a particular form](/introduction-to-utilitarianism#what-is-utilitarianism) of aggregative consequentialism, one that is _welfarist_ and _impartial_: taking well-being to be the only intrinsic value and giving equal weight to everyone’s well-being. Other consequentialist views may weigh different people’s interests differently or count additional things (beyond just well-being) as intrinsically good. Despite these theoretical differences, such views may be considered close cousins of utilitarianism. They tend to be motivated by similar [arguments](/arguments-for-utilitarianism), are subject to similar [objections](/objections-to-utilitarianism), and have broadly similar [practical implications](/acting-on-utilitarianism).

This chapter surveys a range of these related consequentialist accounts, with an eye to assessing (i) why some might prefer them over simple utilitarianism and (ii) the extent to which their practical implications coincide with those of utilitarianism. We close by considering whether even some non-consequentialist theories could end up closely matching utilitarianism in practice.


## Beyond Welfarism

Most would agree that well-being is an important value. Suffering is bad, whereas happiness and human flourishing are good. But is well-being, as utilitarianism claims, the _[only intrinsic value](/types-of-utilitarianism#welfarism)_? This seems more questionable. There is room for reasonable people to judge that other things matter in addition to just well-being. This section explores three possible examples of other candidate goods: environmental value, aesthetic value, and distributive justice.


### Environmental Value

There is an active debate in environmental ethics over whether non-sentient nature—for instance, plants, species, and whole ecosystems—has intrinsic or merely instrumental value. This is difficult to resolve, as our intuitions pull us in different directions.

On the one hand, many find intuitive various welfarist principles, such as the claim that how good an outcome is depends entirely on the well-being of the individuals that exist in that outcome. Or that nothing can be good or bad if there is nobody who exists to care about it or to experience it.

On the other hand, we may be drawn towards certain judgments that seem to attribute intrinsic value to nature. For example, imagine that some disaster wiped out all sentient beings (including all animals) except for a single person. As this last person is on their deathbed, they look over a glorious landscape and face the choice of whether to set off a doomsday device that would cause the planet to explode, destroying all the remaining (non-sentient) plant life.[^1] Even if we stipulate that there was somehow no chance of sentient life ever evolving again on Earth, we may still judge it wrong to wipe out the remaining (non-sentient) plant life for no good reason. But it has (by stipulation) no further instrumental value for sentient beings, so any remaining value it possesses must be intrinsic.

Utilitarians might adapt their [standard response to counterexamples](/objections-to-utilitarianism#general-ways-of-responding-to-objections-to-utiliarianism) by showing how they can accommodate a closely related intuition. They must insist that in this weird scenario (with all its stipulations), it would technically be harmless, and hence permissible, to blow up the planet. But they may endorse our judgment that there would be something wrong with the character of an agent who wanted to blow up the world in this way. The agent would seem disturbingly callous and may be the kind of person who _in more ordinary circumstances_ would risk significant harm by being insufficiently respectful of (immensely instrumentally valuable) natural ecosystems. Anyone unsatisfied by this response may prefer to instead add environmental intrinsic value to their moral theory, yielding a pluralist (rather than welfarist) form of consequentialism.

Some radical environmentalists may take environmental values to outweigh the well-being of all sentient lives, human and non-human alike.[^2] But most would find this position too extreme and instead give more weight to well-being. This more moderate form of pluralism would then largely overlap with utilitarianism in its practical recommendations, except in claiming that small well-being losses can be compensated for by large gains in environmental values. For example, while woodlands may be valuable, the moderate pluralist would agree that in the event of a wildfire, one must always save people before trees.[^3]


### Aesthetic Value

Welfarism may be challenged in a different way by considering our attitudes towards objects of beauty. Consider the awe you feel at observing a magnificent waterfall or pondering the immensity of the universe. It seems _fitting_, or genuinely appropriate, to directly appreciate these things and other objects of beauty—great art, music, natural wonders, etc. “Direct appreciation” here is a form of non-instrumental valuing, recognizably distinct from merely regarding something as instrumentally _useful_. But such non-instrumental valu<em>ing</em> can only be truly warranted by objects that are non-instrumentally valu<em>able</em>.

Welfarists thus seem to face a dilemma: either deny that such eminently appropriate-seeming responses are _in fact_ appropriate or grant that things besides welfare have intrinsic value.

One tempting welfarist response would be to insist that it is really our _appreciative aesthetic experience_ that is valuable, rather than an object such as a waterfall itself. In support of this view, it seems right that the value would be missing from a universe with no conscious observers to appreciate the waterfalls contained therein. But on the other hand, it seems that the appropriate _object_ of our valuing attitudes (that is, what it is that we are in awe of and appropriately regard _as valuable_) is the waterfall _itself_ rather than just our _experience_ of it.[^4]

Can these two verdicts be reconciled? One possibility—for those who accept [an objective list theory of well-being](/theories-of-wellbeing#objective-list-theories)—would be to hold that the waterfall itself is valuable, but only when it is perceived and appreciated, as it may then constitute a welfare good for the subject perceiving it. This is a way to accommodate aesthetic value without abandoning welfarism. A non-welfarist might instead claim that the waterfall has value _independently of its contribution to anyone's well-being_, while nonetheless insisting that this value is only realized when it is perceived and appreciated by a conscious being.

It is ultimately unclear whether aesthetic value requires us to abandon welfarism. Either way, while these details may be of theoretical interest, they seem unlikely to yield significant divergences from utilitarianism in practice (unless one gives implausibly extreme weight to aesthetic value compared to welfarist priorities like relieving suffering and saving lives).


### Egalitarianism and Distributive Justice

Egalitarianism is the view that inequality is bad _in itself_, over and above any instrumental effects it may have on people’s well-being.

Utilitarians tend to favor equality entirely for instrumental reasons—that is, because more equal outcomes tend to better promote overall well-being. Due to the diminishing marginal value of money, for example, an extra dollar makes a much bigger difference to a homeless person than to a billionaire. 

Egalitarians feel that this merely instrumental appreciation of equality does not go far enough. For example, they may prefer a world in which everyone gets fifty years of happy life over an alternative in which an average of sixty years of happy life is achieved by 75% of the population living happily to age 70 while 25% die early at age 30. While this unequal outcome contains (we may suppose) more overall well-being, egalitarians may nonetheless regard it as worse due to the disvalue of inequality.

Note that there are two ways to remedy inequality: one may improve the lot of the worse off, or one may _worsen_ the lot of the better off, to bring everyone down to the same level. The former route is obviously better on well-being grounds. But according to egalitarianism, either route counts equally as an “improvement” as far as just the value of equality is concerned. According to the _Leveling-Down Objection_ to egalitarianism, it counts against the view that it implies there is _something_ good about harming the better off while helping no-one.[^5] Welfarists instead believe that such one-sided harms are _entirely_ bad.[^6]

The article on the [Equality Objection to Utilitarianism](/objections-to-utilitarianism/equality) further discusses the intuition that equality of distribution matters intrinsically and how utilitarians may respond. For now, we may note that substantial practical agreement should be expected between utilitarian and egalitarian views (assuming that the latter appropriately care not merely about [local but global inequality](/utilitarianism-and-practical-ethics#cosmopolitanism), as well as [interspecies inequality](/utilitarianism-and-practical-ethics#speciesism), and [intertemporal inequality](/utilitarianism-and-practical-ethics#longtermism)).


## Beyond the Equal Consideration of Interests

One might accept welfarism while nonetheless rejecting utilitarianism due to giving more weight to the interests or well-being of some individuals than others. This section explores four such views: _prioritarianism_, _desert-adjusted views_, _egoism,_ and _partialism_.


### Prioritarianism

According to prioritarianism, “benefiting people matters more the worse off these people are”.[^7] Like egalitarianism, prioritarianism implies that it is better to give a fixed-size benefit—for instance, an extra boost of happiness—to the worse off than to the better off. But whereas egalitarianism implies that there is _something_ good about “leveling down”, or reducing inequality purely by harming the better off, prioritarianism (like utilitarianism) avoids claiming that there is anything good about harming people in the absence of compensating benefits to others.

This is a subtle but important theoretical difference. Egalitarians value _equality_ per se, which could be promoted by _reducing_ well-being. Prioritarians maintain welfarism, valuing only _well-being_, while departing from utilitarianism by instead giving _extra weight_ to the interests of the worse off.

Prioritarianism may thus seem like a nice compromise between egalitarian intuitions and theoretical principles. But it raises theoretical puzzles of its own:[^8] To bring this out, recall how utilitarians accommodate our egalitarian intuitions by appealing to the _diminishing marginal value_ of resources: the more you have, the less of a difference one more unit tends to make. 

Strangely, prioritarianism treats well-being value itself as having diminishing marginal value. Suppose that Jane has the choice to either provide herself a small benefit at a time when she is poorly off or a greater benefit at a time when she is better-off. By definition, the latter option benefits her more. But prioritarianism implies that the former may be “more important”.[^9] That is, _considering only this person’s welfare, it might be better to do what is worse for her_. Could that really be right?

Utilitarians may agree that our intuitions support prioritarianism but then seek to give a debunking explanation of these intuitions rather than accepting them at face value. Experimental evidence suggests that our intuitive appreciation of the diminishing marginal value of resources overgeneralizes when directly comparing “units of well-being” with which we lack intuitive familiarity.[^10] Joshua Greene argues that “when we imagine possible distributions of [well-being] (...) it’s very hard not to think of distributions of stuff, rather than distributions of experiential quality.”[^11] Utilitarians agree that stuff often has diminishing marginal value. If prioritarian intuitions reflect a confused overgeneralization of this point, we may do best to stick with utilitarianism.

Alternatively, some people might be drawn to the view that various basic goods—such as happiness—that directly contribute to well-being have diminishing marginal value, and then confuse this with the prioritarian claim. To ensure theoretical clarity, we must take care to distinguish the prioritarian idea that the well-being of the worse off simply _matters_ more, from the utilitarian-compatible idea that basic goods like happiness would constitute _a greater benefit_ for the worse off, by making a greater difference to their (inherently equally important) well-being. We might call this new view _diminishing basic goods utilitarianism_, in contrast with the traditional utilitarian view that basic goods like happiness have constant, non-diminishing value. For diminishing basic goods utilitarians, an extra minute of happiness might be intrinsically _more beneficial_ to someone who was previously suffering than to someone who was already fairly happy.[^12] This view seems to yield the same practical verdicts as prioritarianism, but without the theoretical costs.

In any case, as with egalitarianism, the differences between prioritarianism, diminishing basic goods utilitarianism, and traditional utilitarianism are subtle. These differences will be most pronounced in cases where we may benefit the extremely well off at a relatively smaller cost to the least well off. However, such cases are rare (since diminishing returns mean that it is usually much easier to increase the well-being of the have-nots than of the haves). Consequently, all of these views can be expected to share broadly similar practical implications.[^13]


### Desert-Adjusted Views

Utilitarianism counts everyone’s interests equally. An alternative form of consequentialism might involve counting only _innocent_ interests equally, while discounting the interests of those who are, in some relevant sense, less deserving.[^14] For example, it would seem unfair to harm one person to save another from harms that were gratuitously self-inflicted and entirely avoidable.

Many people have _[retributivist](https://plato.stanford.edu/entries/justice-retributive/)_ intuitions, endorsing punishment of the guilty even independently of any instrumental benefits this might have for deterrence or reduced recidivism. Even more would at least think that serious wrongdoers have _less claim_ on receiving benefits from others in society. For example, if you could choose to give an extra happy day to either Gandhi or Hitler, you probably do not feel that you need to flip a coin to decide between them. (You might even prefer for Hitler to positively suffer.)[^15] Desert-adjusted views accommodate such intuitions by making the degree to which one’s interests count morally proportionate to the extent to which one _deserves_ to benefit (or, perhaps, to suffer).

Is it possible for some people to be more deserving of happiness than others? Intuitively, it certainly seems so. But [skeptics about free will and moral responsibility](https://plato.stanford.edu/entries/freewill/#FreeWillMoraResp) argue that this is an illusion, which would preclude any such desert-adjustments. While it is beyond the scope of this article to try to adjudicate that debate here, it is nonetheless worth emphasizing two points. First, _if_ there is such a thing as genuine desert, then consequentialists can happily take this into account—though doing so would be a departure from utilitarianism. And second, even if we end up adopting a desert-adjusted consequentialist view, it will still share the most important practical implications of utilitarianism. After all, there is no serious question about the innocence of those groups we can help the most, such as future generations, factory-farmed animals,[^16] and (the vast majority of) the global poor.


### Egoism and Partialism

Egoists claim that only their _own_ well-being matters, and consequently, they should do whatever is best for themselves.[^17] While some people initially claim to find this “rational”, few philosophers regard it as a defensible view upon reflection.[^18] Most of us care deeply about at least _some_ other people, such as our close friends and family, and would regard it as neither ethical nor rational to benefit ourselves at greater cost to our loved ones.

For example, suppose an evil demon made you the following offer: murder your friends and family, and they will wipe your memory of the past (including the horrendous deed) and set you up in a _new_ life (with new friends and family) in which you are guaranteed to be very slightly happier, more fulfilled, or have marginally more of [whatever else you believe contributes to your well-being](/theories-of-wellbeing). Would it necessarily be _irrational_ to decline? Worse, are you even _required_ to commit these murders? That seems an absurdly strong and implausible claim. Surely you could reasonably care more about the well-being of your friends and family than about marginally improving your own well-being. Selfishly murdering your loved ones for a piddling benefit seems like the epitome of moral error.

So let us put aside egoism. A more plausible view, which we might call _partialism_, lies somewhere between the two extremes of egoism and impartial utilitarianism. According to partialism, while we must take everyone’s interests into account to some extent, we may give _extra_ weight to our nearest and dearest (including ourselves).[^19] 

One important challenge for partialism is that it is unclear _how much_ extra weight partiality allows or how there could even be a precise fact of the matter. Any particular answer would seem arbitrary.[^20] We can make theoretical sense of how everyone might have equal moral weight. But if some degree of partiality is intrinsically (and not just instrumentally) warranted, how could we determine what degree is warranted?

In any case, it at least seems clear that there must be limits to how much extra weight partiality allows. On any plausibly moderate weighting, the standard [utilitarian prescriptions for improving the world at large](/acting-on-utilitarianism) will still apply, given that well-targeted donations can often do _hundreds_ of times more good for distant others than for ourselves and those close to us.[^21]


## Beyond Consequentialism

Some non-consequentialist views—particularly anti-[aggregative](/types-of-utilitarianism#additive-aggregationism) views on which “the numbers do not count”[^22]—are radically at odds with utilitarianism. But many others, while disagreeing deeply about the theoretical role of moral constraints, may nonetheless still largely overlap with utilitarianism in practice.

After all, two main features distinguish standard deontological views from utilitarianism: _options_ to favor oneself and loved ones, and _constraints_, such as against violating rights. The conflict between deontological views with these features and utilitarianism (or consequentialism more broadly) is often overstated. Regarding moral options, the section on partialism above highlighted that any plausibly moderate degree of partiality would not be enough to outweigh the immense good we can do for others. And regarding [moral constraints](/objections-to-utilitarianism/rights), utilitarians wholeheartedly [embrace respecting rights in practice](/utilitarianism-and-practical-ethics#respecting-commonsense-moral-norms), viewing them as instrumentally justified by their tendency to promote better societal outcomes.

Any plausible view must allow that reducing suffering and promoting well-being are important moral goals. It would be extremely implausible to deny this entirely.[^23] But this means that—on _any_ plausible view—we will have strong moral reasons to reduce suffering and promote well-being whenever we can do so without conflicting with other important goals or requirements (such as to respect others’ rights).

In practice, the most effective ways to improve the world do not require violating others’ rights. So it seems that almost any reasonable view that endorses aggregation should end up largely agreeing with utilitarianism about [what matters in practice](/acting-on-utilitarianism): namely, pursuing effective ways to vastly improve the world[^24] while behaving in a trustworthy fashion, advancing social coordination, and respecting important moral constraints.


## Conclusion

There are several ways to reject aspects of utilitarianism while remaining on board with the general thrust of the theory (at least in practice). First, one could add other values into the mix, so long as they are not taken to such extremes as to completely swamp the importance of sentient creatures’ well-being. Second, one could reject the equal consideration of interests, giving extra weight to the worse off, the more deserving, or those to whom one is partial. But again, as long as such weightings are not taken to implausible extremes (such as strict egoism), the moral imperative to seek effective improvements to the world will remain in force. Finally, one might even reject consequentialism and accept fundamental moral constraints on one’s pursuit of the good. But many such non-consequentialists may still allow that the good remains worthy of pursuit nonetheless (as long as that pursuit does not require violating important moral constraints), and many utilitarians will likely approve of their proposed constraints in practice.

Of course, there are ways to reject utilitarianism wholesale. One might embrace egoism and deny that others matter at all. Or one might reject aggregation and insist that helping millions is no more intrinsically important than helping one. (Or that helping millions, each a little, cannot compare to the moral importance of helping one person a lot.) Anyone drawn to these views is unlikely to sympathize with utilitarianism or its practical recommendations.

But those are a fairly narrow range of views. By contrast, it is noteworthy—and to many readers, possibly quite surprising—what a broad range of theoretical approaches may happily form a “coalition” with utilitarianism to advance [effective altruism](/acting-on-utilitarianism#effective-altruism).

The next chapter discusses several of the most important objections to utilitarianism and responses to these objections.

{{< next-page-textbook title="Objections to Utilitarianism and Responses" url="/objections-to-utilitarianism" >}}

{{< how-to-cite authors="Chappell, R.Y. and Meissner, D." >}}

{{< button >}}

## Resources and Further Reading

### Beyond Welfarism

* Richard Routley (1973). [Is there a need for a new, an environmental, ethic?](https://doi.org/10.5840/wcp151973136). _Proceedings of the XVth World Congress of Philosophy_, 1: 205–210.
* Elizabeth Anderson (1999). [What is the Point of Equality?](https://doi.org/10.1086/233897) _Ethics_ 109 (2): 287–337.
* Derek Parfit (1997). [Equality and Priority](https://dx.doi.org/10.1111/1467-9329.00041). _Ratio_, 10 (3): 202–221.

### Prioritarianism

* Derek Parfit (1997). [Equality and Priority](https://dx.doi.org/10.1111/1467-9329.00041). _Ratio_, 10(3): 202–221.
* Johan E. Gustafsson (2021). _[Ex-Ante Prioritarianism Violates Sequential Ex‑Ante Pareto](https://doi.org/10.1017/S0953820821000303)_. _Utilitas,_ 1–11.
* Joshua Greene & Jonathan Baron (2001). [Intuitions about Declining Marginal Utility](https://dx.doi.org/10.2139/ssrn.231183). _Journal of Behavioral Decision Making_, 14: 243–55.

### Desert-Adjusted Views

* Fred Feldman (1995). [Adjusting utility for justice: A consequentialist reply to the objection from justice](https://dx.doi.org/10.2307/2108439). _Philosophy and Phenomenological Research_, 55(3): 567–585.

### Egoism and Partialism

* Simon Keller (2013). _Partiality_. Princeton University Press.
* Derek Parfit (1984). Part Two: Rationality and Time, _[Reasons and Persons](https://en.wikipedia.org/wiki/Reasons_and_Persons)_. Clarendon Press.
* Andreas Mogensen (2019). [The only ethical argument for positive 𝛿?](https://globalprioritiesinstitute.org/andreas-mogensen-the-only-ethical-argument-for-positive-delta-2/). forthcoming in _Philosophical Studies_.
* Troy Jollimore (2018). [Impartiality](https://plato.stanford.edu/entries/impartiality/ )_. The Stanford Encyclopedia of Philosophy_. Edward N. Zalta (ed.).
* Robert Goodin (1988). [What is so special about our fellow countrymen?](https://dx.doi.org/10.1086/292998) _Ethics_ 98(4): 663–686.

### Beyond Consequentialism

* John Taurek (1977). [Should the numbers count?](https://www.jstor.org/stable/2264945). _Philosophy and Public Affairs_, 6(4): 293–316.
* David Ross (1930). _The Right and the Good_. Clarendon Press.
* Elizabeth Ashford (2003) [The Demandingness of Scanlon's Contractualism](https://doi.org/10.1086/342853). _Ethics_ 113(2): 273–302.


[^1]:
     This is a variation on Richard Routley’s famous “Last Man” thought experiment. See Routley, R. (1973). [Is there a need for a new, an environmental, ethic?](https://doi.org/10.5840/wcp151973136) _Proceedings of the XVth World Congress of Philosophy_, 1: 205–210.

[^2]:
     Some claim that humanity is overall harmful for the world, such that we should wish for human extinction. But it is important to note that this does not actually follow: for even if humanity has _to date_ imposed immense costs on the natural environment, it may well be that _future_ generations of humanity are the best hope for undoing that damage. The near-term extinction of humanity might then be the worst possible outcome: all our past damage is done, but none of our future potential for remedy could then be realized. If that is right, then even radical environmentalists should care (at least instrumentally) about preventing human extinction.

[^3]:
     Pluralists face interesting puzzles about how to make trade-offs involving _vastly more_ of the typically less important value. Could a million trees outweigh a human life? Or outweigh a sufficiently tiny _risk _of death to one person? We will not attempt to solve this problem here, but note it as an important challenge for any pluralist view.

[^4]:
     Note that a subjectively indistinguishable hallucinatory experience (perhaps generated by [an experience machine](/theories-of-wellbeing#the-experience-machine-objection)) does not seem quite so impressive. Of course, if the person does not realize they are hallucinating, they will feel just as impressed. But we may judge their valuing attitude to be objectively less warranted in such a case than in the non-hallucinatory case.

[^5]:
     Parfit, D. (1997). [Equality and Priority](https://dx.doi.org/10.1111/1467-9329.00041). _Ratio_, 10(3): 202–221.

[^6]:
     This is a subtle disagreement, with limited practical implications, so long as the egalitarian sensibly agrees that the well-being losses from leveling down make it _overall_ bad.

[^7]:
     Parfit, D. (1997). [Equality and Priority](https://dx.doi.org/10.1111/1467-9329.00041). _Ratio_, 10(3): 202–221, p. 213.

[^8]:
     The following paragraphs directly draw from Chappell, R.Y. (2021). _[Parfit’s Ethics](https://doi.org/10.1017/9781108582377). _Cambridge University Press. Note that Chappell is a co-author of this website.

[^9]:
     We assume, for simplicity, that prioritarianism applies to momentary rather than lifetime well-being. For the latter view, the objection must be tweaked to involve counterfactual rather than temporal comparisons. Suppose that Shane will be happier if a flipped coin lands heads, and can further grant himself either of two conditional benefits: a greater benefit if the coin lands heads, or a small benefit if tails. If benefits matter more to the worse off, and he is worse off if it lands tails, then the 50% chance of a smaller benefit (conditional on tails) may be recommended by prioritarianism as better than the 50% chance of a greater benefit. But that would not be the prudent choice. This assumes that prioritarianism takes an _ex post_ form. For objections to _ex ante_ prioritarianism, see Gustafsson, J. (2021). _[Ex-Ante Prioritarianism Violates Sequential Ex‑Ante Pareto](https://doi.org/10.1017/S0953820821000303). Utilitas_, 1-11.

[^10]:
     Greene, J. and Baron, J. (2001). [Intuitions about Declining Marginal Utility](https://dx.doi.org/10.2139/ssrn.231183). _Journal of Behavioral Decision Making_, 14: 243–55.

[^11]:
     Cf. Greene, J. (2013). _[Moral Tribes: Emotion, Reason, and the Gap Between Us and Them](https://www.joshua-greene.net/moral-tribes)_. New York: The Penguin Press. Chapter 10.

[^12]:
     Note that this is not the empirical claim that happiness would have greater downstream (instrumental) benefits for the worse-off person, say by shifting them into a more positive frame of mind. That may or may not be true, but if it is then it would be taken into account even by traditional utilitarians. No, diminishing basic goods utilitarians make the more radical normative claim that the same amount of pleasure inherently _constitutes_ a greater _intrinsic_ benefit to the worse-off person. The pleasure _in itself _makes a bigger difference to their well-being, in the sense that we have prudential reason to more strongly prefer a fixed-size boost to our pleasure or happiness when we are worse-off.

[^13]:
     A possible exception might be that if you assigned _extreme_ priority to the worse off, this could lead to inefficiently pouring resources into hard-to-reach or difficult-to-help populations for minor gains, when utilitarians would instead recommend using those resources to get greater benefits for larger numbers of (possibly slightly better-off) people.

[^14]:
     Feldman, F. (1995). [Adjusting utility for justice: A consequentialist reply to the objection from justice](https://dx.doi.org/10.2307/2108439). _Philosophy and Phenomenological Research_, 55(3): 567–585.
     Graham, P. A. (2020). [Avoidable Harm](https://dx.doi.org/10.1111/phpr.12586). _Philosophy and Phenomenological Research_ 101 (1):175-199.

[^15]:
     [Jeremy Bentham](/utilitarian-thinker/jeremy-bentham), by contrast, assigned no less _intrinsic_ value even “to the most abominable pleasure which the vilest of malefactors ever reaped from his crime.” See Bentham, J. (1789). Chapter 2: Principles Opposing the Principle of Utility, _[An Introduction to the Principles of Morals and Legislation](https://www.earlymoderntexts.com/assets/pdfs/bentham1780.pdf)_. Bennett, J. (ed.), 2017.

[^16]:
     An alternative view that would allow some discounting of non-human animals would be to weigh interests in proportion to the moral status of the individual (and further hold that non-human animals have lesser moral status). While [utilitarians already allow](/utilitarianism-and-practical-ethics#speciesism) that some animals may have a greater capacity for conscious experiences than others, this alternative view would instead claim that even equal pains matter less if experienced by a “lesser” animal. But even with moderate discounting, the severe harms of factory farming are unlikely to prove justifiable. For we surely should not discount animals so much that it would become permissible to torture them for fun.

[^17]:
     Ethical egoists claim that you _morally_ should do whatever is best for yourself. Rational egoists claim that you _rationally_ ought to do whatever is best for yourself. But here we take egoism to be the view that _all things considered_, what you _really _ought to do is whatever is best for yourself. This claim is likely to be endorsed by both ethical and rational egoists.

[^18]:
     For classic discussion, see Derek Parfit (1984). Part Two: Rationality and Time, _[Reasons and Persons](https://en.wikipedia.org/wiki/Reasons_and_Persons)_. Clarendon Press.

[^19]:
     Philosophers call this an _option_ or _agent-centered prerogative_. See Scheffler, S. (1994). _[The Rejection of Consequentialism](https://oxford.universitypressscholarship.com/view/10.1093/0198235119.001.0001/acprof-9780198235118)_. Oxford University Press.

[^20]:
     One option would be to accept a vague range of permissible weightings, but this can begin to look mysterious.

[^21]:
     Mogensen further argues that long-term considerations will swamp pretty much _any_ finitely weighted personal prerogative, even ones that initially seem extremely immoderate (such as giving a _million_ times more weight to the interests of those to whom you are partial). See: Mogensen, A. (forthcoming). [Moral Demands and the Far Future](https://doi.org/10.1111/phpr.12729). _Philosophy and Phenomenological Research_.

[^22]:
     E.g., Taurek, J. (1977). [Should the numbers count?](https://www.jstor.org/stable/2264945) _Philosophy and Public Affairs_, 6(4): 293–316.

[^23]:
     Even John Rawls, the father of contractualist ethics and a prominent critic of utilitarianism, remained committed to the core aspect of consequentialism, writing that “All ethical doctrines worth our attention take consequences into account in judging rightness. One which did not would simply be irrational, crazy”. 
    Rawls, J. (2009). _A Theory of Justice (Revised Edition)_. Harvard University Press., p. 26

[^24]:
     One important proviso: deontologists may be more drawn to [person-affecting views of population ethics](/population-ethics#person-affecting-views-and-the-procreative-asymmetry), which could result in their giving much less weight to the interests of future generations.
